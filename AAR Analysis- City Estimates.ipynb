{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ca067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Import the AAR Data\n",
    "file_path = '/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Test/AAR.xlsx'\n",
    "df_cargo = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae71fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage for each row excluding the 'Commodity' column\n",
    "df_percentage = df_cargo.iloc[:, 1:].div(df_cargo.iloc[:, 1:].sum(axis=1), axis=0) * 100\n",
    "df_percentage.insert(0, 'Commodity', df_cargo['Commodity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151589a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Census Data\n",
    "file_path_block_census = '/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/pdb2023bg- Planning Group Track - Indiana Filtered.xlsx'\n",
    "df_census = pd.read_excel(file_path_block_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613823eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Occupational Data\n",
    "file_path_occupation = '/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/ACSST5Y2022.S2405_2024-10-27T152508/ACSST5Y2022.S2405-Data_edited_3.xlsx'\n",
    "df_occupation = pd.read_excel(file_path_occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5663b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GIDTR in df_census to string\n",
    "df_census['GIDTR'] = df_census['GIDTR'].astype(str)\n",
    "\n",
    "# Extract the last 11 digits from GEO_ID \n",
    "df_occupation['GIDTR'] = df_occupation['Geography'].str[-11:].astype(str)\n",
    "\n",
    "# Merge the two DataFrames on the GIDTR column\n",
    "merged_df = pd.merge(df_census, df_occupation, on='GIDTR', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d41b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of industry columns \n",
    "industry_columns = [\n",
    "    'Agriculture, forestry, fishing and hunting, and mining',\n",
    "    'Construction',\n",
    "    'Manufacturing',\n",
    "    'Wholesale trade',\n",
    "    'Retail trade',\n",
    "    'Transportation and warehousing, and utilities',\n",
    "    'Information',\n",
    "    'Finance and insurance, and real estate and rental and leasing',\n",
    "    'Professional, scientific, and management, and administrative and waste management services',\n",
    "    'Educational services, and health care and social assistance',\n",
    "    'Arts, entertainment, and recreation, and accommodation and food services',\n",
    "    'Other services, except public administration',\n",
    "    'Public administration'\n",
    "]\n",
    "\n",
    "# Calculate the total number of workers for each industry across all tracts\n",
    "industry_totals = merged_df[industry_columns].sum()\n",
    "\n",
    "# Calculate the percentage of workers in each industry for each census tract\n",
    "for column in industry_columns:\n",
    "    percentage_column = f\"{column}_Percentage\"  # New column name for percentage\n",
    "    merged_df[percentage_column] = merged_df[column] / industry_totals[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2743d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of commodities to their relevant industries\n",
    "industry_commodity_map = {\n",
    "    'Agriculture, forestry, fishing and hunting, and mining': ['Farm Products', 'Food Products', 'Coal', 'Primary Metal Products'],\n",
    "    'Construction': ['Primary Metal Products'],\n",
    "    'Manufacturing': ['Chemicals', 'Petroleum', 'Coal', 'Primary Metal Products'],\n",
    "    'Wholesale trade': ['Other'],\n",
    "    'Retail trade': ['Other'],\n",
    "    'Transportation and warehousing, and utilities': ['Coal', 'Primary Metal Products'],\n",
    "    'Professional, scientific, and management, and administrative and waste management services': ['Waste and Scrap'],\n",
    "    'Arts, entertainment, and recreation, and accommodation and food services': ['Food Products'],\n",
    "}\n",
    "\n",
    "# Initialize columns for each commodity's distribution\n",
    "for commodity in df_cargo.columns[1:]: \n",
    "    merged_df[f'{commodity}_distributed'] = 0\n",
    "\n",
    "# Loop through each commodity in df_cargo to distribute\n",
    "for commodity in df_cargo.columns[1:]:\n",
    "    relevant_industries = [industry for industry, com_list in industry_commodity_map.items() if commodity in com_list]\n",
    "    relevant_industry_population = merged_df[relevant_industries].sum(axis=1).sum()\n",
    "    merged_df[f'{commodity}_percentage'] = merged_df[relevant_industries].sum(axis=1) / relevant_industry_population\n",
    "    merged_df[f'{commodity}_distributed'] = merged_df[f'{commodity}_percentage'] * df_cargo[commodity][0]\n",
    "\n",
    "# Drop the temporary percentage columns\n",
    "percentage_columns = [f'{commodity}_percentage' for commodity in df_cargo.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4f5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import List of Latitude and Longitude Coordinates for each census track\n",
    "file_path_coor = '/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Test/Coordinates.xlsx'\n",
    "df_coor = pd.read_excel(file_path_coor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d877e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both 'GIDTR' and 'GEOID' to strings\n",
    "merged_df['GIDTR'] = merged_df['GIDTR'].astype(str)\n",
    "df_coor['GEOID'] = df_coor['GEOID'].astype(str)\n",
    "\n",
    "# Merge the coordinate data into the data set\n",
    "merged_df = pd.merge(merged_df, df_coor[['GEOID', 'Latitude', 'Longitude']], \n",
    "                              left_on='GIDTR', right_on='GEOID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd88645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SVI Data\n",
    "file_path_svi = '/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/Team/Indiana_censusTract.csv'\n",
    "df_svi = pd.read_csv(file_path_svi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b87336e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both columns are the same type (e.g., string)\n",
    "merged_df['GEOID'] = merged_df['GEOID'].astype(str)\n",
    "df_svi['FIPS'] = df_svi['FIPS'].astype(str)\n",
    "\n",
    "# Merging SVI data into the data set\n",
    "merged_df = merged_df.merge(df_svi[['FIPS', 'RPL_THEMES']], \n",
    "                             left_on='GIDTR', \n",
    "                             right_on='FIPS', \n",
    "                             how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fa6c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e27b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6163f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "#Primary Metal Distribution\n",
    "# Create the base map\n",
    "m = folium.Map(location=[40.0, -85.0], zoom_start=7)\n",
    "\n",
    "# Create a marker cluster\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# Loop through the data and add markers to the cluster\n",
    "for _, row in merged_df.iterrows():\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    value = row['Primary Metal Products_distributed']\n",
    "    \n",
    "    # Create a popup with value information\n",
    "    popup = folium.Popup(f\"Value: {value:.2f}\", max_width=300)\n",
    "    \n",
    "    # Add the marker with a circle marker icon\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=value / 10,  # Adjust the divisor for desired size\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        popup=popup\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save(\"primary_metal_products_clustered_map.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bed1c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coal Distribution\n",
    "# Loop through the data and add markers to the cluster\n",
    "for _, row in merged_df.iterrows():\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    value = row['Coal_distributed']\n",
    "    \n",
    "    # Create a popup with value information\n",
    "    popup = folium.Popup(f\"Value: {value:.2f}\", max_width=300)\n",
    "    \n",
    "    # Add the marker with a circle marker icon\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=value / 10,  # Adjust the divisor for desired size\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        popup=popup\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save(\"Coal_distributed_products_clustered_map.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e14d759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Farm Products\n",
    "# Loop through the data and add markers to the cluster\n",
    "for _, row in merged_df.iterrows():\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    value = row['Farm Products_distributed']\n",
    "    \n",
    "    # Create a popup with value information\n",
    "    popup = folium.Popup(f\"Value: {value:.2f}\", max_width=300)\n",
    "    \n",
    "    # Add the marker with a circle marker icon\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=value / 10,  # Adjust the divisor for desired size\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        popup=popup\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save(\"Farm Products_distributed_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c55d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Food Products\n",
    "# Loop through the data and add markers to the cluster\n",
    "for _, row in merged_df.iterrows():\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    value = row['Food Products_distributed']\n",
    "    \n",
    "    # Create a popup with value information\n",
    "    popup = folium.Popup(f\"Value: {value:.2f}\", max_width=300)\n",
    "    \n",
    "    # Add the marker with a circle marker icon\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=value / 10,  # Adjust the divisor for desired size\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        popup=popup\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save(\"Food Products_distributed.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16ddbb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chemical Products\n",
    "# Loop through the data and add markers to the cluster\n",
    "for _, row in merged_df.iterrows():\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    value = row['Chemicals_distributed']\n",
    "    \n",
    "    # Create a popup with value information\n",
    "    popup = folium.Popup(f\"Value: {value:.2f}\", max_width=300)\n",
    "    \n",
    "    # Add the marker with a circle marker icon\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=value / 10,  # Adjust the divisor for desired size\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        popup=popup\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save(\"Chemicals_distributed.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9057945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other\n",
    "# Loop through the data and add markers to the cluster\n",
    "for _, row in merged_df.iterrows():\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    value = row['Other_distributed']\n",
    "    \n",
    "    # Create a popup with value information\n",
    "    popup = folium.Popup(f\"Value: {value:.2f}\", max_width=300)\n",
    "    \n",
    "    # Add the marker with a circle marker icon\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=value / 10,  # Adjust the divisor for desired size\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        popup=popup\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save(\"Other_distributed.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc73a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Waste and Scrap\n",
    "# Loop through the data and add markers to the cluster\n",
    "for _, row in merged_df.iterrows():\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    value = row['Waste and Scrap_distributed']\n",
    "    \n",
    "    # Create a popup with value information\n",
    "    popup = folium.Popup(f\"Value: {value:.2f}\", max_width=300)\n",
    "    \n",
    "    # Add the marker with a circle marker icon\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=value / 10,  # Adjust the divisor for desired size\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        popup=popup\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save(\"Waste and Scrap_distributed.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f62999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Petroleum\n",
    "# Loop through the data and add markers to the cluster\n",
    "for _, row in merged_df.iterrows():\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    value = row['Petroleum_distributed']\n",
    "    \n",
    "    # Create a popup with value information\n",
    "    popup = folium.Popup(f\"Value: {value:.2f}\", max_width=300)\n",
    "    \n",
    "    # Add the marker with a circle marker icon\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=value / 10,  # Adjust the divisor for desired size\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        popup=popup\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save(\"Petroleum_distributed.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb2100",
   "metadata": {},
   "source": [
    "The below data analysis uses the files found in the City Estimates file and are used to estimate terminated and originated commodities shipped throughout the US. This includes cargo that is terminated, originated and pass through the state of Indiana. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d156d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/Team/tl_2024_us_county.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"tl_2024_us_county\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f4f759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = gpd.read_file(\"tl_2024_us_county/tl_2024_us_county.shp\")\n",
    "counties.to_file('counties.geojson', driver = 'GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af72d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_df = pd.read_csv('/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/Team/Indiana_Rail_FAF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f34f09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate all data with a destination in Indiana\n",
    "#USING 2022 as 2023 estimates are not yet finalized\n",
    "cmd_dms_orig_df = cmd_df.groupby(['dms_orig', 'sctg2']).\\\n",
    "                agg(\n",
    "                    {'tons_2022': 'sum'})\\\n",
    "                .reset_index()\\\n",
    "                .rename(columns = {'dms_orig' : 'dms_loc'})\n",
    "\n",
    "cmd_dms_orig_df = cmd_dms_orig_df[(cmd_dms_orig_df['dms_loc']>= 180) & (cmd_dms_orig_df['dms_loc'] <= 189)].copy(deep = True)\n",
    "\n",
    "#Aggregate all dadta with an origin from Indiana\n",
    "cmd_dms_dest_df = cmd_df.groupby(['dms_dest', 'sctg2'])\\\n",
    "                .agg(\n",
    "                    {'tons_2022': 'sum'})\\\n",
    "                .reset_index()\\\n",
    "                .rename(columns = {'dms_dest' : 'dms_loc'})\n",
    "\n",
    "cmd_dms_dest_df = cmd_dms_dest_df[(cmd_dms_dest_df['dms_loc']>= 180) & (cmd_dms_dest_df['dms_loc'] <= 189)].copy(deep = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f7fdd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine datasets to create city level commodity shares\n",
    "cmd_dms_loc_df = pd.concat([cmd_dms_orig_df, cmd_dms_dest_df], ignore_index=True)\n",
    "cmd_dms_ref_df = cmd_dms_loc_df.groupby('dms_loc').agg({'tons_2022': 'sum'})\\\n",
    "                .reset_index()\\\n",
    "                .rename(columns = {'tons_2022': 'total_tons_2022'})\n",
    "\n",
    "\n",
    "cmd_dms_loc_df = pd.merge(cmd_dms_loc_df, cmd_dms_ref_df, on = 'dms_loc', how = 'left')\n",
    "\n",
    "cmd_dms_loc_df['loc_cmd_share'] = cmd_dms_loc_df['tons_2022']/cmd_dms_loc_df['total_tons_2022']\n",
    "\n",
    "#City level commodity shares to be distributed to the city and collar counties?\n",
    "#Preliminary logic below, but needs to be combined with employment data methodology and structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86f2514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_Counties = counties[counties['STATEFP'] == '18'].copy(deep = True)\n",
    "\n",
    "#Primary and Collar Counties for listed cities\n",
    "FW_Cnt = ['Allen', 'Wells', 'Whitley', 'Huntington', 'Adams']\n",
    "Gary_Cnt = ['Lake', 'Porter']\n",
    "Ind_Cnt = ['Marion', 'Boone', 'Hamilton', 'Madison', 'Hancock', 'Shelby', 'Johnson', 'Morgan', 'Hendricks']\n",
    "\n",
    "FW_df = IN_Counties[IN_Counties['NAME'].apply(lambda x: any(s in x for s in FW_Cnt))]\n",
    "Gary_df = IN_Counties[IN_Counties['NAME'].apply(lambda x: any(s in x for s in Gary_Cnt))]\n",
    "Ind_df = IN_Counties[IN_Counties['NAME'].apply(lambda x: any(s in x for s in Ind_Cnt))]\n",
    "\n",
    "IN_Counties['Commodity_Mapping'] = np.where(IN_Counties['NAME'].str.contains('|'.join(FW_Cnt), case = False), 'Fort Wayne IN',\n",
    "                                            np.where(IN_Counties['NAME'].str.contains('|'.join(Gary_Cnt), case = False), 'Chicago IL-IN-WI (IN Part)',\n",
    "                                                     np.where(IN_Counties['NAME'].str.contains('|'.join(Ind_Cnt), case = False), 'Indianapolis IN',\n",
    "                                                              'Rest of IN')))\n",
    "\n",
    "IN_Counties['dms_code'] = np.where(IN_Counties['NAME'].str.contains('|'.join(FW_Cnt), case = False), 183,\n",
    "                                            np.where(IN_Counties['NAME'].str.contains('|'.join(Gary_Cnt), case = False), 181,\n",
    "                                                     np.where(IN_Counties['NAME'].str.contains('|'.join(Ind_Cnt), case = False), 182,\n",
    "                                                              189)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43e3e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/Team/FAF5.6.1_2018-2023/FAF5.6.1_2018-2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d6bad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in refference dfs, filter down to just rail\n",
    "city_df = pd.read_excel('/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/Team/FAF5.6.1_2018-2023/FAF5_metadata.xlsx', sheet_name = 'FAF Zone (Domestic)', header= 0)\n",
    "comm_type_df = pd.read_excel('/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/Team/FAF5.6.1_2018-2023/FAF5_metadata.xlsx', sheet_name = 'Commodity (SCTG2)', header= 0)\n",
    "dist_df = pd.read_excel('/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/Team/FAF5.6.1_2018-2023/FAF5_metadata.xlsx', sheet_name = 'Distance Band', header= 0)\n",
    "rail_df = data_df[data_df['dms_mode'] == 2].copy(deep = True)\n",
    "indiana_codes = [181, 182, 183, 189]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf784952",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = city_df[['Numeric Label', 'Short Description']].rename(columns= {'Numeric Label': 'dms_orig', 'Short Description': 'dms_orig_desc'})\n",
    "dest_df = city_df[['Numeric Label', 'Short Description']].rename(columns= {'Numeric Label': 'dms_dest', 'Short Description': 'dms_dest_desc'})\n",
    "comm_type_df = comm_type_df.rename(columns= {'Numeric Label': 'sctg2', 'Description' : 'Commodity_Type'})\n",
    "dist_df = dist_df.rename(columns = {'Numeric Label': 'dist_band', 'Description' : 'Distance_Band'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4f595b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rail_df = rail_df[rail_df['dms_orig'].isin(indiana_codes) | rail_df['dms_dest'].isin(indiana_codes)]\n",
    "final_df = pd.merge(rail_df, orig_df, on = 'dms_orig', how = 'left')\n",
    "final_df = pd.merge(final_df, dest_df, on = 'dms_dest', how = 'left')\n",
    "final_df = pd.merge(final_df, comm_type_df, on = 'sctg2', how = 'left')\n",
    "final_df = pd.merge(final_df, dist_df, on = 'dist_band', how = 'left')\n",
    "final_df = final_df.drop(columns = ['fr_orig', 'fr_dest', 'fr_inmode', 'fr_outmode'])\n",
    "final_df.to_csv('Indiana_Rail_FAF.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a44fe2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge IN_Counties and cmd_dms_loc_df\n",
    "cmd_dms_loc_df_2 = pd.merge(\n",
    "    cmd_dms_loc_df,                # Left DataFrame\n",
    "    IN_Counties[['dms_code', 'Commodity_Mapping','NAMELSAD']],  # Columns to merge from IN_Counties\n",
    "    left_on='dms_loc',             # Key column in cmd_dms_loc_df\n",
    "    right_on='dms_code',           # Key column in IN_Counties\n",
    "    how='left'                     # Use left join to retain all rows in cmd_dms_loc_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69fa0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Commodity Code Data set\n",
    "file_path_code = '/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/Team/Commodity_Codes.xlsx'\n",
    "df_FAF_codes = pd.read_excel(file_path_code)  # Specify the sheet name if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9635983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge cmd_dms_loc_df and df_FAF_codes\n",
    "cmd_dms_loc_df_3 = pd.merge(\n",
    "    cmd_dms_loc_df_2,                # Left DataFrame\n",
    "    df_FAF_codes[['Code', 'Commodity','AAR Category']],  \n",
    "    left_on='sctg2',             \n",
    "    right_on='Code',        \n",
    "    how='left' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0da3daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data to make AAR Category commodity types into columns\n",
    "commodity_pivot = cmd_dms_loc_df_3.pivot_table(\n",
    "    index='NAMELSAD',                 # Rows will be counties\n",
    "    columns='AAR Category',          # Columns will be commodity types\n",
    "    values='tons_2022',              # Values to aggregate\n",
    "    aggfunc='sum',                   # Summing weights of commodities\n",
    "    fill_value=0                     # Replace NaN with 0\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns for clarity (optional, flatten MultiIndex columns)\n",
    "commodity_pivot.columns.name = None  # Remove the pivot table column grouping\n",
    "commodity_pivot = commodity_pivot.rename_axis(None, axis=1)  # Remove row name\n",
    "\n",
    "# Merge back into the original dataset\n",
    "cmd_dms_loc_df_4 = pd.merge(\n",
    "    cmd_dms_loc_df_3,\n",
    "    commodity_pivot,\n",
    "    on='NAMELSAD',\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28ee34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the first instance of each county based on NAMELSAD to reduce redundancy\n",
    "cmd_dms_loc_df_5 = cmd_dms_loc_df_4.drop_duplicates(subset='NAMELSAD', keep='first')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8386c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into main data set cmd_dms_loc_df_5\n",
    "merged_df_2 = pd.merge(\n",
    "    merged_df,                # Left DataFrame\n",
    "    cmd_dms_loc_df_5[['NAMELSAD', 'Chemicals', 'Coal', 'Farm Products', 'Food Products', \n",
    "    'Other', 'Petroleum', 'Primary Metal Products', 'Waste and Scrap','total_tons_2022']],  \n",
    "    left_on='County_name',            \n",
    "    right_on='NAMELSAD',          \n",
    "    how='left'                 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a3afd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Pass through data\n",
    "pass_through_df = pd.read_csv('/Users/allenho/Documents/Grad IU/Fall 2024/Data Science In Practice/Freight/Data Sets/Team/ind_passthrough_estimates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c175b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge pass through data df_FAF_codes\n",
    "pass_through_df_2 = pd.merge(\n",
    "    pass_through_df,                # Left DataFrame\n",
    "    df_FAF_codes[['Code', 'Commodity','AAR Category']],  \n",
    "    left_on='sctg2',             \n",
    "    right_on='Code',        \n",
    "    how='left' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7845ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAR Category    counties_crossed    Chemicals         Coal  Farm Products  \\\n",
      "0                   Adams County  1480.530256  1069.396004    3902.589636   \n",
      "1                   Allen County  1984.946953  1195.269354    4674.351890   \n",
      "2             Bartholomew County   907.043141  1427.010352    6617.535316   \n",
      "3                  Benton County  1358.743113  4436.743365     147.353922   \n",
      "4               Blackford County   861.405525   972.980436     845.196035   \n",
      "..                           ...          ...          ...            ...   \n",
      "87             Washington County   692.967569   368.724177    1948.462812   \n",
      "88                  Wayne County  1038.474108   290.780509    3216.943544   \n",
      "89                  Wells County  1668.807401  3469.115316    3729.636148   \n",
      "90                  White County  2023.268293  1771.994637     610.003549   \n",
      "91                Whitley County   703.590281   329.469680    1001.133691   \n",
      "\n",
      "AAR Category  Food Products        Other    Petroleum  Primary Metal Products  \\\n",
      "0                200.759101  2432.959519  2426.056318             3599.830427   \n",
      "1                795.838589  3505.134892  4740.829107             2949.027751   \n",
      "2                328.279120  1473.611160   673.583675             1063.556854   \n",
      "3                915.097849  1218.486685   757.094202             2366.232045   \n",
      "4                803.865785  6618.995274  2393.533048             2000.511401   \n",
      "..                      ...          ...          ...                     ...   \n",
      "87               740.575180  1702.985878  1172.003869              745.596323   \n",
      "88              1594.253016  1480.152480   326.623978              202.348468   \n",
      "89               333.720210  3288.167038  1374.288728             2164.500990   \n",
      "90               743.240479  6147.781072  2769.767699             2346.930173   \n",
      "91               478.654830  1307.609143  2537.603803             1183.477020   \n",
      "\n",
      "AAR Category  Waste and Scrap  \n",
      "0                  626.923654  \n",
      "1                 3974.917029  \n",
      "2                   43.061264  \n",
      "3                  189.024004  \n",
      "4                 2158.547240  \n",
      "..                        ...  \n",
      "87                  36.874955  \n",
      "88                 317.151630  \n",
      "89                1381.359821  \n",
      "90                2008.371262  \n",
      "91                 600.726459  \n",
      "\n",
      "[92 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#Cleaning pass through data to find on a per county basis the amount in each category\n",
    "pass_through_expanded = pass_through_df_2.copy()\n",
    "\n",
    "\n",
    "pass_through_expanded['counties_crossed'] = pass_through_expanded['counties_crossed'].str.split(',')\n",
    "pass_through_expanded = pass_through_expanded.explode('counties_crossed')\n",
    "\n",
    "# Remove unwanted characters such as brackets and quotes, and strip any leading/trailing whitespace\n",
    "pass_through_expanded['counties_crossed'] = pass_through_expanded['counties_crossed']\\\n",
    "    .str.replace(r\"[\\[\\]']\", \"\", regex=True)\\\n",
    "    .str.strip()\n",
    "\n",
    "#Add word County for df merge later\n",
    "pass_through_expanded['counties_crossed'] = pass_through_expanded['counties_crossed'].apply(\n",
    "    lambda x: x if \"County\" in x else f\"{x} County\"\n",
    ")\n",
    "\n",
    "\n",
    "aggregated_df = pass_through_expanded.groupby(['counties_crossed', 'AAR Category'], as_index=False)['tons_2022'].sum()\n",
    "result_df = aggregated_df.pivot(index='counties_crossed', columns='AAR Category', values='tons_2022')\n",
    "result_df = result_df.fillna(0)\n",
    "result_df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a93912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge into main datasetthe above\n",
    "merged_df_3 = pd.merge(\n",
    "    merged_df_2,                # Left DataFrame\n",
    "    result_df[['counties_crossed', 'Chemicals', 'Coal', 'Farm Products', 'Food Products', \n",
    "    'Other', 'Petroleum', 'Primary Metal Products', 'Waste and Scrap']],  \n",
    "    left_on='County_name',            \n",
    "    right_on='counties_crossed',          \n",
    "    how='left'                     # Use left join to retain all rows in cmd_dms_loc_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad43a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to sum from pass through data\n",
    "columns_to_sum = [\n",
    "    'Chemicals_y', 'Coal_y', 'Farm Products_y', 'Food Products_y', \n",
    "    'Other_y', 'Petroleum_y', 'Primary Metal Products_y', 'Waste and Scrap_y'\n",
    "]\n",
    "\n",
    "#List of columns to sum from terminated and originated data\n",
    "columns_to_sum_term = [\n",
    "    'Chemicals_x', 'Coal_x', 'Farm Products_x', 'Food Products_x', \n",
    "    'Other_x', 'Petroleum_x', 'Primary Metal Products_x', 'Waste and Scrap_x'\n",
    "]\n",
    "\n",
    "merged_df_3['total_tons_2022_y'] = merged_df_3[columns_to_sum].sum(axis=1)\n",
    "\n",
    "# Calculate percentages for each column and add new columns for pass through data\n",
    "for col in columns_to_sum:\n",
    "    merged_df_3[f'{col}_pct'] = (merged_df_3[col] / merged_df_3['total_tons_2022_y']) * 100\n",
    "    \n",
    "\n",
    "# Calculate percentages for each column and add new columns for terminated data\n",
    "for col in columns_to_sum_term:\n",
    "    merged_df_3[f'{col}_pct'] = (merged_df_3[col] / merged_df_3['total_tons_2022']) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3ceb8",
   "metadata": {},
   "source": [
    "The below calculates a risk score for commodity data. As it currently stands the weight is distributed equally. Commodity categories with _distributed is derived from census data on worker distribution and original AAR categories. Categories with _x are derived from terminated/oringated data, and those denoted with _y are derived from pass through data. You can adjust the weight distribution as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2181f838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_combined_risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.653406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.365178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.533756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.364302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.371572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>3.304667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>4.672357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>4.939028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>3.027639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>3.603178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1696 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_combined_risk\n",
       "0                5.653406\n",
       "1                4.365178\n",
       "2                5.533756\n",
       "3                5.364302\n",
       "4                5.371572\n",
       "...                   ...\n",
       "1691             3.304667\n",
       "1692             4.672357\n",
       "1693             4.939028\n",
       "1694             3.027639\n",
       "1695             3.603178\n",
       "\n",
       "[1696 rows x 1 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define risk weights for each commodity\n",
    "risk_weights = {\n",
    "    'Chemicals': 5,\n",
    "    'Coal': 3,\n",
    "    'Farm Products': 1,\n",
    "    'Food Products': 1,\n",
    "    'Other': 2,\n",
    "    'Petroleum': 4,\n",
    "    'Primary Metal Products': 2,\n",
    "    'Waste and Scrap': 4\n",
    "}\n",
    "\n",
    "# List of commodities\n",
    "commodities = [\n",
    "    'Chemicals', 'Coal', 'Farm Products', 'Food Products', \n",
    "    'Other', 'Petroleum', 'Primary Metal Products', 'Waste and Scrap'\n",
    "]\n",
    "\n",
    "# Weight for each dataset\n",
    "dataset_weight = 1/3\n",
    "\n",
    "# Initialize the total risk score column\n",
    "merged_df_3['total_combined_risk'] = 0\n",
    "\n",
    "# Calculate the combined risk score for each county\n",
    "for commodity in commodities:\n",
    "    # Get column names for each dataset\n",
    "    distributed_col = f\"{commodity}_distributed\"\n",
    "    terminated_col = f\"{commodity}_x\"\n",
    "    pass_through_col = f\"{commodity}_y\"\n",
    "    \n",
    "    # Calculate normalized risk for each dataset\n",
    "    distributed_risk = (merged_df_3[distributed_col] / merged_df_3[distributed_col].sum()) * dataset_weight\n",
    "    terminated_risk = (merged_df_3[terminated_col] / merged_df_3[terminated_col].sum()) * dataset_weight\n",
    "    pass_through_risk = (merged_df_3[pass_through_col] / merged_df_3[pass_through_col].sum()) * dataset_weight\n",
    "    \n",
    "    # Combine risks for the commodity and weight by risk factor\n",
    "    combined_risk = (distributed_risk + terminated_risk + pass_through_risk) * risk_weights[commodity]\n",
    "    \n",
    "    # Add to total combined risk score\n",
    "    merged_df_3['total_combined_risk'] += combined_risk\n",
    "\n",
    "# Scale the total combined risk score between 1 and 10\n",
    "min_risk = merged_df_3['total_combined_risk'].min()\n",
    "max_risk = merged_df_3['total_combined_risk'].max()\n",
    "\n",
    "merged_df_3['total_combined_risk'] = 1 + (merged_df_3['total_combined_risk'] - min_risk) / (max_risk - min_risk) * 9\n",
    "\n",
    "# Display the resulting dataset\n",
    "merged_df_3[['total_combined_risk']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "268b4a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jh/7v3ft64505b60fvqz7vsgpz40000gn/T/ipykernel_3365/3992824761.py:1: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  merged_df_3.to_excel(\"AAR_Analysis_City_Estimates.xlsx\", index=False)\n"
     ]
    }
   ],
   "source": [
    "merged_df_3.to_excel(\"AAR_Analysis_City_Estimates.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f9186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
